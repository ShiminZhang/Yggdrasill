## Goal

This paper aims to give a through and concrete overview of Recurrent Neural Networks with precision.

It first introduces the origin and intuition behand RNN, which is to learn long range dependencies on sequencial tasks.

Then, it explains the early model of RNN and points out the problem with it. The early version of RNN has two problems, gradient descent and gradient explode. Some efforts are taken to alleviate those problem, including variations of activation functions.

The next part is about modern RNN. Naturally, modern RNN evolves to cope with gradient problems. This paper explains the structure and modifications of LSTM, BRNN and Neural Turing Machine.

The last part focuses on applications of RNN and the correspondence between different RNN structures and real world applications.

## GPT polish up !

This manuscript endeavors to furnish a thorough and precise overview of Recurrent Neural Networks (RNNs), aiming for scholarly rigor.

Initially, it delineates the genesis and underlying rationale of RNNs, which are designed to capture long-range dependencies in sequential tasks.

Subsequently, the discourse transitions to the early RNN models, highlighting inherent shortcomings, specifically, the issues of vanishing and exploding gradients. It elucidates the various strategies employed to mitigate these challenges, including the adaptation of diverse activation functions.

The narrative progresses to contemporary advancements in RNN architecture. Modern RNNs have evolved to address gradient-related issues effectively. This document elaborates on the architecture and enhancements of Long Short-Term Memory (LSTM) networks, Bidirectional RNNs (BRNNs), and Neural Turing Machines (NTMs).

The concluding section is dedicated to the practical applications of RNNs, emphasizing the synergy between distinct RNN architectures and their real-world implementations.